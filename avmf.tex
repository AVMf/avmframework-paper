\documentclass{llncs}

% needed for algorithms
\usepackage[ruled]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xspace}

\newcommand{\AVM}{Alternating Variable Method\xspace}
\newcommand{\longname}{AVM Framework\xspace}
\newcommand{\name}{AVM\hspace{-1pt}$f$\xspace}
\newcommand{\repourl}{\url{https://github.com/AVMf/avmf}\xspace}
\newcommand{\gitclone}{{\tt git clone https://github.com/AVMf/avmf.git}\xspace}
\newcommand{\inlineheading}[1]{\vspace{1ex} \noindent {\bf #1.}}
\newcommand{\codescalefactor}{0.7}

% for algorithms
\newcommand{\mf}{\mathit{mf}}

\newcommand{\snippet}[1]{
	\begin{center}
		\scalebox{\codescalefactor}{
			\begin{tabular}{|l|}
			\hline
			\input{snippets/#1}
			\hline
			\end{tabular}
		}
	\end{center}
}

\begin{document}

\title{\name: An\:Open-Source\:Framework\:and\\$\!$Implementation\:of\:the\:Alternating\:Variable\:Method\vspace{-.5em}}

%\author{Phil McMinn\inst{1} \and Gregory M. Kapfhammer\inst{2} \and Dirk Sudholt\inst{1} \and Sonal Mahajan\inst{3} \and William G.J. Halfond\inst{3}}

%\institute{University of Sheffield, UK \and Allegheny College, USA \and University of Southern California, USA}

% aim: get author block to fit in this space (or less)
\author{
	Suppressed\\
	for\\
	\vspace{1mm} \small{double-blind review}
	\vspace{-2em}
}
\institute{}

\maketitle

\begin{abstract}
The \AVM (AVM) has been shown to be a particularly fast and effective local search technique in search-based software engineering. %, particularly test data generation.
%
Recent developments to the AVM have generalized the types of representations it can optimize and have provably improved its runtime for certain types of fitness landscape. Until now, however, there has been no publicly-available code implementation of these algorithms. We introduce \name, an object-oriented Java framework that provides such an implementation. \name is ready for download and configuration for use in SBSE projects.

\end{abstract}

\section{Introduction}

\vspace{-.5ex}
The {\it \AVM} (AVM) was first proposed for search-based software engineering by Korel in 1990~\cite{Korel1990} as an adaptation of so-called ``direct'' search methods \cite{Gill1974,Glass1965} for the generation of software test data.
% The AVM is an adaptation of so-called direct search methods \cite{Gill1974,Glass1965} to the test data generation problem.
% In particular, the AVM that did not make assumptions about the branching constraints of the program --- for example, their linearity and continuity.
Despite the application of supposedly more robust, global search techniques to test data generation (e.g., Genetic Algorithms), the AVM has stood the test of time. In 2007, Harman and McMinn~\cite{Harman2007} reported its effectiveness and efficiency for a series of C programs, and combined it with a GA to provide a ``best of'' Memetic Algorithm approach~\cite{Harman2010}. It has since been used to generate test suites for Java programs~
\cite{%Fraser2013,
Fraser2015b}, having been implemented into the {\sc EvoSuite} tool, generate rows of data for testing databases~\cite{Kapfhammer2013,McMinn2015} with the {\it SchemaAnalyst} tool, and has been combined with dynamic symbolic approaches in Microsoft's Pex tool \cite{Lakhotia2010}.
It has also been applied to problems outside of test data generation, for example test case selection~\cite{Pradhan2016}.

Since its initial application, various different extensions have been made to the algorithm in terms of the types of variables it can handle (namely floating-point types~\cite{Harman2007} and strings~\cite{McMinn2015}), along with the incorporation of different strategies that are proven to speed up the search for certain common types of objective function landscape
\cite{%Kempka2013,
Kempka2015}.

The AVM is therefore capable of handling a variety of search representations and locating solutions to SBSE problems in a very efficient manner. Nevertheless, to incorporate it into an SBSE project, an SBSE researcher has previously had to understand the different variants of algorithm and then produce a faithful implementation --- a potentially time-consuming and error prone task. To address this problem, we have developed \name, an open source object-oriented framework that implements variants of the AVM and its representations, and is available for download and instant deployment in SBSE projects. \name~is fully documented and comes with a series of examples as to its usage.

%This paper is organized as follows: first we introduce the basic form of the AVM, as originally proposed by Korel~\cite{Korel1990} (Section~\ref{sec:basic}). We then survey some of the improvements subsequently made (Section~\ref{sec:improvements}). Section~\ref{sec:avmf} then introduces \name, our AVM framework, describing how it is implemented and how it is intended to be used. Finally, Section~\ref{sec:conclusions} concludes with proposals as to future projects to which the framework could be applied.

% Before introducing \name, we briefly review the basic form of the AVM algorithm and recent extensions.

\section{The Basic Form of the AVM}
\label{sec:basic}

% something about how the algorithm starts, randomly or with initial values?

The original AVM used by Korel~\cite{Korel1990} takes a vector $\vec{x} = (x_1, \dots, x_n)$, and cycles through each element of the vector one by one as shown by Algorithm~\ref{basic}, invoking a specific local search to improve each one in turn. The local search of the original AVM, referred to as ``Iterated Pattern Search'' (IPS), is shown in Algorithm~\ref{alg:ips}. The initial part of the search corresponds to so-called {\it exploratory} moves. A small increment is made to the variable followed by a small decrement. If either of the moves yields an improvement in the objective value for the complete value, a positive or negative ``direction'' has been established. IPS seeks to make further moves in the direction, referred to as {\it pattern} moves. Consecutive pattern moves double in size, so long as the previous move improved the objective value. In this manner, IPS gets closer and closer to the target value until it likely ``overshoots'' the target, due to a pattern move that was larger than the difference between the current position and the goal value. When this occurs, IPS reestablishes a new direction with two more exploratory moves. If neither move leads to an improvement in objective value, the local search terminates and hands back control the main loop.

\begin{algorithm}[b]
	\small
	\algsetup{linenosize=\small}
		\caption{The main loop of the AVM for optimizing $\vec{x} = (x_1, \dots, x_n)$ % \mbox{(taken from Kempka et al. \cite{Kempka2015})}
		}
	\begin{algorithmic}[1]
		\WHILE{\textbf{true}}
			\STATE \textbf{let} $\vec{x}:=\mathbf{random}()$
			\STATE \textbf{let} $i:=1$, $c:=0$
			\WHILE{$c<n$}
				\STATE \textbf{let} $f \colon x \mapsto \mf(x_1, \dots, x_{i-1}, x, x_{i+1}, \dots, x_n)$
				\STATE \textbf{let} $\vec{x'}:=\mathbf{local\_search}(f,x_i)$
				 \IF{$\mf(\vec{x'})<\mf(\vec{x})$}
					\STATE \textbf{let} $\vec{x}:=\vec{x'}$, $c:=0$
				\ELSE
					\STATE \textbf{let} $c:=c+1$
				\ENDIF
				\STATE \textbf{let} $i:= (i \bmod n) +1$
			\ENDWHILE
		\ENDWHILE
	\end{algorithmic}
	\label{alg:basic}
\end{algorithm}

\begin{algorithm}[b]
	\footnotesize
	\algsetup{linenosize=\scriptsize}
	\caption{$\text{Iterated Pattern Search, starting at }x\in D$}
	\begin{algorithmic}[1]
		\WHILE{$\textbf{true}$}
			\STATE$\textbf{if }f(x-1)\geq f(x)\textbf{ and }f(x+1)\geq f(x)\textbf{ return }x$
			\STATE$\textbf{if }f(x-1)<f(x+1)\textbf{ then let }k:=-1\textbf{ else let }k:=1$
			\WHILE{$f(x+k)<f(x)$}
				\STATE$\textbf{let }x:=x+k\text{, }k:=2k$
			\ENDWHILE
		\ENDWHILE
	\end{algorithmic}
	\label{alg:ips}
\end{algorithm}

If a complete cycle of the vector occurs without any improvement in the objective function, the search has hit a local optimum, at which point it restarts by randomizing the vector.

This continues until resources are exhausted (e.g., a maximum number of objective function evaluations or restarts is reached, or a time limit has expired), or the optimal target vector is discovered. (For simplicity, these termination criteria are not included as part of the algorithm definitions.)



\section{Improvements to the AVM}
\label{sec:improvements}

\inlineheading{Representations}
The original AVM was only demonstrated for integer variables. Harman and McMinn~\cite{Harman2007} extended this definition by allowing each variable to have a precision setting $p$, allowing floating-point values to be handled. Exploratory moves then consisted of the smallest value of the variable according to $p$ (i.e., $10^{-p}$). McMinn et al.~\cite{McMinn2015} allowed string variables to be represented by the approach. A string variable is essentially a sub-vector, whose elements are characters that are individually manipulated by the local search routine. The length of this sub-vector is allowed to vary through a special sequence of moves that increase and decrease its size, allowing for the optimization of variable-length strings.

\inlineheading{Algorithms}
Kempka et al.~\cite{Kempka2015} proposed two new local searches for the AVM, and proved that are more efficient in unimodal landscapes. The first, ``Geometric'' search is shown by Algorithm~\ref{alg:geometric}. Geometric search performs exploratory moves followed by pattern moves, if a direction of improvement can be established. Following pattern moves, it does not return to exploratory moves, but instead ``brackets'' the upper and lower limits in which the local optima must lie, and performs a binary search.

Kempka et al.~\cite{Kempka2015} also proposed ``Lattice'' search, which is shown to be even faster than Geometric search. Lattice search is similar to Geometric search, except that after bracketing the position of the local optimum, converges on the target values using Fibonacci numbers, as shown by Algorithm~\ref{alg:lattice}.

\begin{algorithm}[t]
	\footnotesize
	\algsetup{linenosize=\scriptsize}
	\caption{$\text{Geometric search, starting at }x\in D$}
	\begin{algorithmic}[1]
		\STATE$\textbf{if }f(x-1)\geq f(x)\textbf{ and }f(x+1)\geq f(x)\textbf{ return }x$
		\STATE$\textbf{if }f(x-1)<f(x+1)\textbf{ then let }k:=-1\textbf{ else let }k:=1$
		\WHILE{$f(x+k)<f(x)$}
			\STATE$\textbf{let }x:=x+k\text{, }k:=2k$
		\ENDWHILE
		\STATE$\textbf{let }\ell:=\min(x-k/2,x+k)\text{, }r:=\max(x-k/2,x+k)$
		\WHILE{$\ell<r$}
			\IF{$f(\lfloor(\ell+r)/2\rfloor)<f(\lfloor(\ell+r)/2\rfloor+1)$}
				\STATE$r:=\lfloor(\ell+r)/2\rfloor$
			\ELSE
				\STATE$\ell:=\lfloor(\ell+r)/2\rfloor+1$
			\ENDIF
		\ENDWHILE
		\RETURN$\ell$
	\end{algorithmic}
	\vspace*{1mm}
	\label{alg:geometric}
\end{algorithm}

\begin{algorithm}[t]
	\footnotesize
	\algsetup{linenosize=\scriptsize}
	\caption{$\text{Lattice search, starting at }x\in D$}
	\begin{algorithmic}[1]
		\STATE$\textbf{if }f(x-1)\geq f(x)\textbf{ and }f(x+1)\geq f(x)\textbf{ return }x$
		\STATE$\textbf{if }f(x-1)<f(x+1)\textbf{ then let }k:=-1\textbf{ else let }k:=1$
		\WHILE{$f(x+k)<f(x)$}
			\STATE$\textbf{let }x:=x+k\text{, }k:=2k$
		\ENDWHILE
		\STATE$\textbf{let }\ell:=\min(x-k/2,x+k)\text{, }r:=\max(x-k/2,x+k)$
		\STATE$\textbf{let }n:=\min\{n\mid F_{n}\geq r-l+2\}$
		\WHILE{$n>3$}
			\IF{$\ell+F_{n-1}-1\leq r\textbf{ and }f(\ell+F_{n-2}-1)\geq f(\ell+F_{n-1}-1)$}
				\STATE$\textbf{let }\ell:=\ell+F_{n-2}$
			\ENDIF
			\STATE$\textbf{let }n:=n-1$
		\ENDWHILE
		\RETURN$\ell$
	\end{algorithmic}
	\vspace*{1mm}
	\label{alg:lattice}
\end{algorithm}



\section{The \longname (\name)}
\label{sec:avmf}
The \longname (\name) is an object-oriented framework and Java implementation of the AVM and its enhancements since the original version proposed by Korel.  The framework has been implemented to make the core algorithms behind the AVM and its use as clear as possible, closely matching paper definitions supplied in Algorithms~\ref{alg:basic}--\ref{alg:lattice} (i.e., through the use of similar variable names and decision structures etc.) while adhering to good principles of object-oriented design.

It is publicly available for download at \repourl (a GitHub-hosted Git repository) for inclusion in SBSE projects, where the AVM may be the core search algorithm or be a component of a more complex one (e.g., a memetic algorithm) to which calls to the framework are made. Or, the code can simply be lifted from the repository and re-used and adapted as researchers see fit.

~\\
\name provides a framework of Java classes to enable the algorithms to be easily used in SBSE projects, which we now describe in detail below. Each aspect of the framework is practically demonstrated in the source code of the examples introduced at the end of this section.

\begin{sloppypar}
\inlineheading{Configuring an AVM search} The primary class is the {\tt AVM} class in the root ({\tt org.avmframework}) package. In order to construct an {\tt AVM} instance, the developer must supply an instance of one of the local search methods; {\tt IteratedPatternSearch}, {\tt GeometricSearch} or {\tt LatticeSearch}, which reside in the {\tt localsearch} package. The developer must also construct the {\tt AVM} instance with a {\tt TerminationPolicy}, an object that decides when the AVM should terminate, for example if an optimal solution is not found. Options include a maximum number of objective function evaluations, a maximum number of restarts, or a time threshold. Finally, constructing the {\tt AVM} instance requires instances of {\tt Initializer} that decide from where the algorithm starts the search and restarts the search, be it from the default values that can be specified for each variable or from a random position ({\tt DefaultInitializer} and {\tt RandomInitializer} respectively, which reside in the {\tt initializer} package). For random values, \name uses an instance of {\tt RandomGenerator} from the {\tt org.apache.commons} library. (All the examples provided with \name, described above, use the library's implementation of the Mersenne Twister algorithm.)
\end{sloppypar}

\begin{sloppypar}
In order to initiate a search process, the {\tt search} method of the {\tt AVM} instance must be invoked with an instance of a {\tt Vector} class and an {\tt ObjectiveFunction} respectively. The {\tt Vector} class describes the {\it representation} of the problem to the AVM, that is, the series of variables to be optimized; while the {\tt ObjectiveFunction} class describes how instances of those vectors should be rewarded. We now describe these aspects in further detail.
\end{sloppypar}

\begin{sloppypar}
\inlineheading{Representation} In order to configure the search representation, an instance of the {\tt Vector} class (in the root package) must be created, and variables added to it through the {\tt addVariable} method, which accepts an instance of a {\tt Variable}. {\tt Variable} is an abstract class, so an instance of one its concrete subclasses must be provided --- {\tt IntegerVariable}, {\tt FloatingPointVariable}, {\tt CharacterVariable} or {\tt StringVariable}. Each variable needs to be constructed with information such as its minimum or maximum value (maximum length for strings), precision in case of floating point variables, and a ``default'' initial value in the search space (e.g., the empty string or zero values). This values are used to set the variables to specific values if the {\tt DefaultInitializer} is used to start the search, as previously described.
\end{sloppypar}

\begin{sloppypar}
\inlineheading{Objective Function} In contrast to the rest of the framework, which require configuring instances of existing classes, an objective function is supplied to the search process by overriding the abstract {\tt ObjectiveFunction} (of the {\tt objective} package). This involves providing an implementation of the {\tt computeObjectiveValue} method, which takes a {\tt Vector} as a parameter and returns an instance of an {\tt ObjectiveValue}. Since the AVM only needs to know whether one class has a ``better'' objective value than another, exact numerical values are not needed, and so this class only provides ``betterThan'', ``worseThan'' and ``sameAs'' methods. This is useful for when a problem involves two objectives that must be optimized, but with some precedence (e.g., the classical ``approach level'' and ``branch distance'' components of the fitness function for structural test data generation \cite{}), avoiding the need for a normalization function to combine the values. % Arcuri, Poulding GECCO reference.
Nevertheless, the {\tt objective} package also supplies a concrete {\tt NumericalObjectiveValue} class for returning higher-is-better or lower-is-better numerical objective values.
\end{sloppypar}

\begin{sloppypar}
\inlineheading{Reporting} The {\tt search} method of the {\tt AVM} class returns an instance of the {\tt Monitor} class, which can be used to find out interesting statistics regarding the search. These include the best vector found by the search, its objective value, the number of objective function evaluations that took place, the number of restarts that took place, and the amount of time that the search took (in milliseconds). The {\tt Monitor} class can also report the number of {\it unique} objective function evaluations. The objective function can make optional usage of a cache that maps previously ``seen'' vectors to objective values (a feature that is turned on by default). Using this cache helps make the search process more efficient by not repeating potentially expensive objective function evaluations for vectors that have already been considered during the search.
\end{sloppypar}

\inlineheading{Examples} \name comes with three small examples demonstrating its use. (Instructions on how to compile and run these examples are available in the project's {\tt README.md} file.) The first, {\tt Quadratic} demonstrates the use of the AVM to solve a quadratic equation by finding one of its roots. The {\tt AllZeroes} example demonstrates the optimization of an array of integers to zero values, from arbitrary random values. Finally, {\tt String} optimizes a string value from an initially random string to the value ``{\tt \AVM}''. The examples can be configured with a command line parameter to use IPS, Geometric or Lattice search as the local search method.

Each example makes use of its own problem-specific fitness function that can be seen as part of its code definition. The example below is taken from the {\tt Quadratic} class, where the constants {\tt A}, {\tt B} and {\tt C} correspond to the co-efficients of the equation (in the example {\tt A}\,{\tt =}\,{\tt 4}, {\tt B}\,{\tt =}\,{\tt 10} and {\tt C}\,{\tt =}\,{\tt 6}). The function obtains the value of {\tt x} from the (single-variable) vector, and computes the value of {\tt y}. The objective value is then assigned as the distance between {\tt y} and zero, since intuitively, the closer the value of {\tt y} to zero, the closer the search is to finding one of the roots of the equation.

\snippet{objective-fn}

The following is the output of the search process, showing how the AVM search correctly found one of the roots, $-1.5$:

\snippet{search-output}

Re-running the search from a different starting point may lead to the other root, $-1$, being found.

\section{Conclusions}
\label{sec:conclusions}
This paper introduced \name, an open source implementation of the AVM and framework for its use in SBSE projects. Possible future applications of the AVM include the following:

\inlineheading{Automatically Generating Readable Test Data}
Generating {\it readable} test cases that humans can easily understand has been of interest to researchers in search-based testing of late (e.g., rewarding inputs that obtain a high score from a language model \cite{Afshan2013}). More readable values were requested by participants of a recent study evaluating test generation tools \cite{Fraser2013b}. Given that the AVM is local search, the technique could be used to start with examples of human generated inputs and adapt them to new coverage targets without necessarily losing the qualities of the original data.

\inlineheading{Automatically Determining Optimal Software Configuration Values}
Highly configurable pieces of software, such as the GCC compiler, that may be tunable through search techniques such as the AVM. In large spaces of parameters, the AVM is well suited to quickly discovering which particular variables are relevant to the problem through its exploratory move phase, and then proceeding to determining their optimal values. Again, as a local search technique, the AVM is also well suited to taking an existing known-good human solution and improving upon it.

\inlineheading{Automated Bug-Fixing}
Bugs can occur as a result of errors when programming constants and settings files, and as such the AVM could be used as a technique for searching for appropriate values that could potentially form the basis of a ``fix''. Through its exploratory move phase, the AVM is well-suited to quickly determining which constants are ``relevant'' to the fix, by performing a quick sweep of small changes through the values involved, and seeing how the resulting fitness values are affected.

%% Commented out for double blind review
%\inlineheading{Acknowledgment}
%We thank Joseph Kempka for an initial implementation of Geometric and Lattice search that we used to test our own version against.

% References
\bibliographystyle{splncs03}
\bibliography{refs}

\end{document}
